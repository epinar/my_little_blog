<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FoundPose: Unseen Object Pose Estimation with Foundation Features">
  <meta name="keywords" content="FoundPose, unseen pose">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FoundPose: Unseen Object Pose Estimation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><b>FoundPose</b>: Unseen Object Pose Estimation with Foundation Features</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://evinpinar.github.io">Evin Pınar Örnek</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://ylabbe.github.io">Yann Labbé</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://btekin.github.io">Bugra Tekin</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/lingnima/">Lingni Ma</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/cem-keskin-23692a15">Cem Keskin</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cforster.ch">Christian Forster</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://cmp.felk.cvut.cz/~hodanto2/">Tomas Hodan</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Technical University of Munich,</span>
            <span class="author-block"><sup>2</sup>Reality Labs at Meta</span>
          </div>
          <br>
          <div class="column has-text-centered">
            <div class="publication-links">
              
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2311.18809"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03742.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Publication</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03742-supp.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                    <i class="fas fa-file-alt"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/facebookresearch/foundpose"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <div style="text-align: center;">
      <div class="image-container mb-5">
      <img src="./static/images/teaser_fig.png" alt="FoundPose">
    </div>
      </div>
      <h2 class="title"> Abstract</h2>
      <p>
        We propose FoundPose, a model-based method for 6D pose estimation of unseen objects from a single RGB image. The method can quickly onboard new objects using their 3D models without requiring any object- or task-specific training.  In contrast, existing methods typically pre-train on large-scale, task-specific datasets in order to generalize to new objects and to bridge the image-to-model domain gap.
        We demonstrate that such generalization capabilities can be observed in a recent vision foundation model trained in a self-supervised manner. Specifically, our method estimates the object pose from image-to-model 2D-3D correspondences, which are established by matching patch descriptors from the recent DINOv2 model between the image and pre-rendered object templates. We find that reliable correspondences can be established by kNN matching of patch descriptors from an intermediate DINOv2 layer. Such descriptors carry stronger positional information than descriptors from the last layer, and we show their importance when semantic information is ambiguous due to object symmetries or a lack of texture. To avoid establishing correspondences against all object templates, we develop an efficient template retrieval approach that integrates the patch descriptors into the bag-of-words representation and can promptly propose a handful of similarly looking templates. Additionally, we apply featuremetric alignment to compensate for discrepancies in the 2D-3D correspondences caused by coarse patch sampling. The resulting method noticeably outperforms existing RGB methods for refinement-free pose estimation on the standard BOP benchmark with seven diverse datasets and can be seamlessly combined with an existing render-and-compare refinement method to achieve RGB-only state-of-the-art results.
      </p> <br>
    <h2 class="title">BibTeX</h2>
    <pre class="pre-wrap"><code>@article{ornek2024foundpose,
  author    = {{\"O}rnek, Evin P{\i}nar and Labb\'e, Yann and Tekin, Bugra and Ma, Lingni and Keskin, Cem and Forster, Christian and Hoda{\v{n}}, Tom{\'a}{\v{s}}},
  title     = {FoundPose: Unseen Object Pose Estimation with Foundation Features}, 
  journal   = {ECCV},
  year      = {2024},
}</code></pre>
    </div>
  </div>
</section>


</body>
</html>
