---
layout: post
title: Some Papers from ICLR 2020, mostly CV
categories: [research]
comments: true
---

ICLR this year happened fully virtual thanks to coronavirus, and I had a chance to participate as a volunteer student. Normally it was planned to be in Ethiopia to improve the inclusivity culture by letting more people in Africa to attend. Yet I think it was even more inclusive in this way, especially that the registration fees were much reduced, volunteering option for students, and the conference was only a laptop and an internet away. 

<img src="/images/iclr.png">

In the virtual conference, instead of poster sessions, there were 5 minute presentation videos from all authors, supported by 4 hour long Live QA sessions on Zoom. As a first year phd student, it was extremely useful for me where I had to chance ask many questions and learn a lot. 

This year's ICLR was dominated by graph neural networks, reinforcement learning and universal language models. There were 687 papers in total. In this post, I want to share some of them, and mostly the vision relevant ones. Fortunately, poster presentations are public so you can take a look at them!

### [Semantically-Guided Representation Learning for Self-Supervised Monocular Depth](https://iclr.cc/virtual/poster_ByxT7TNFvH.html)

They pretrain a semantic labelling network, and fuse the frozen weights with the depth prediction layers in an attention-like manner. This is useful when there are not many semantic labeled data (they train semantic labeling on Cityscapes, but depth prediction on KITTI). 
Also, to solve the infinite depth problem which occurs due to objects that move at the same speed with camera, they train model at two steps. First is the normal full training, second one is the refinement which only considers the frames that don't have such a problem. To select such frames, they backproject the depth on 3D and remove the points that lie below the horizon.

### [Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving](https://iclr.cc/virtual/poster_BJedHRVtPB.html)

They first have a stereo depth map prediction module to predict 3D objects in the scene. Yet, only image based models are not usually as accurate as expensive Lidar sensors. Instead of a Lidar, it is possible to use a low cost sensor and a way to use the sparse sensor data along with images. In this paper, authors first estimate the depth map, and backproject on 3D scene. They learn the neighborhood relationship between the points by building k-Nearest Neighborhood map. Then, they relocate the points that match with the sensor readings, whereas the remaining unmatched point locations are updated by using the previously learned neighborhood map.

### [End to End Trainable Active Contours via Differentiable Rendering](https://iclr.cc/virtual/poster_rkxawlHKDr.html) 

They predict segmentation contours by using polygons as a middle representation. They first initialize points as a circle shaped polygon-contour. Given an image, through a trained autoencoder, they predict the amount of shift on the initial polygon: two maps that show the displacements of points on x direction an y direction. They update the initial polygon points (contour) according to the shifts, apply Delaunay triangulation to get mask regions. By feeding these triangle faces into a neural renderer, the final mask is extracted (neural renderer is not trained, it is used only to make it end-to-end learnable). The predicted mask is then compared to ground truth and that loss is used to train all model. 

### [RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis](https://iclr.cc/virtual/poster_HyxjNyrtPr.html) 

The idea is to train a generator to generate RGB image and depth pair with given camera parameters (K, R, T). Besides GAN discriminator loss which checks if products are realistic, they generate two images with two different poses, and use an additional photoconsistency loss by warping the first to second and second to the first one. They show how this can be used along with PCGAN, StyleGAN and DeepVoxels.

### [Higher-Order Function Networks for Learning Composable 3D Object Representations](https://iclr.cc/virtual/poster_HJgfDREKDB.html) 

The task is single image point cloud reconstruction. Assuming there is a decoder (mapper) which takes points from a canonical space and maps to points on an object(independent from the object), they create an "encoder" network which takes an image as input, and produces a latent vector which will actually become decoder's parameters. So actually encoder learns to generate decoder parameters depending on the input image. They minimize the chamfer distance on reconstructed point sets. According to the experiments, their model works better than previously posed "nearest neighbor object retrieval" issue on 3D reconstruction.    

### [A Critical Analysis of Self-Supervision, or What We Can Learn from a Single Image](https://iclr.cc/virtual/poster_B1esx6EYvr.html)

They train a self-supervised network (eg. DeepCluster or BiGAN) with a single image cropped in 1M pieces of different sizes and augmentation. They compare this network with 1M different images trained version, and a fully supervised trained model. By taking layers at each level (conv1, conv2, conv3), they compare the classification accuracy when only that layer is used. They found that the low-level accuracies were similar in 1-img, 1M-img, and supervised models. However, the difference between them increased when higher network layer is used. It shows that: low level info can be captured even by a well augmented single image, whereas the current supervised models cannot capture high-level info good even though they use 1M images. 


Other interesting vision papers:

* [Image-guided Neural Object Rendering](https://iclr.cc/virtual/poster_Hyg9anEFPS.html)
* [Neural Outlier Rejection for Self-Supervised Keypoint Learning](https://iclr.cc/virtual/poster_Skx82ySYPH.html)
* [DeepV2D: Video to Depth with Differentiable Structure from Motion](https://iclr.cc/virtual/poster_HJeO7RNKPr.html)
* [Unpaired Point Cloud Completion on Real Scans using Adversarial Training](https://iclr.cc/virtual/poster_HkgrZ0EYwB.html)
* [Deep 3D Pan via Local adaptive "t-shaped" convolutions with global and local adaptive dilations](https://iclr.cc/virtual/poster_B1gF56VYPH.html)
* [Scale-Equivariant Steerable Networks](https://iclr.cc/virtual/poster_HJgpugrKPS.html)
* [Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping](https://iclr.cc/virtual/poster_BJxt60VtPr.html)


Object-centric Scene Modelling:

* [Contrastive Learning of Structured World Models](https://iclr.cc/virtual/poster_H1gax6VtDB.html): They model the environments that consist multiple objecst in a structured way, where each object state in the coming image is represented as a vector feature, the object relationships are captured with Graph Neural Network. 
* [SCALOR: Generative World Models with Scalable Object Representations](https://iclr.cc/virtual/poster_SJxrKgStDH.html)
* [GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations](https://iclr.cc/virtual/poster_BkxfaTVFwH.html)
* [SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition](https://iclr.cc/virtual/poster_rkl03ySYDH.html)


And many interesting stuff on GNNs, disentanglement, RL, meta-learning:

* [On the Equivalence between Positional Node Embeddings and Structural Graph Representations](https://iclr.cc/virtual/poster_SJxzFySKwH.html)
* [Disentangling Factors of Variations Using Few Labels](https://iclr.cc/virtual/poster_SygagpEKwB.html)
* [A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms](https://iclr.cc/virtual/poster_ryxWIgBFPS.html)
* [Learning to Guide Random Search](https://iclr.cc/virtual/poster_B1gHokBKwS.html)
* [Improving Generalization in Meta Reinforcement Learning using Learned Objectives](https://iclr.cc/virtual/poster_S1evHerYPr.html)
* [Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards](https://iclr.cc/virtual/poster_SJg5J6NtDr.html)

Learning on different topologies, self-supervision:

* [Mixed-curvature Variational Autoencoders](https://iclr.cc/virtual/poster_S1g6xeSKDS.html)
* [B-Spline CNNs on Lie groups](https://iclr.cc/virtual/poster_H1gBhkBFDH.html)
* [From Variational to Deterministic Autoencoders](https://iclr.cc/virtual/poster_S1g7tpEYDS.html)
* [DeepSphere: a graph-based spherical CNN](https://iclr.cc/virtual/poster_B1e3OlStPB.html)
* [Geom-GCN: Geometric Graph Convolutional Networks](https://iclr.cc/virtual/poster_S1e2agrFvS.html)
* [Self-labelling via simultaneous clustering and representation learning](https://iclr.cc/virtual/poster_Hyx-jyBFPr.html)
* [Network Deconvolution](https://iclr.cc/virtual/poster_rkeu30EtvS.html)




