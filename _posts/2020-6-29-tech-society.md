---
layout: post
title: Societal Challenges in the Age of Information
categories: [artificial intelligence]
comments: true
---

Computers and internet communication became the driving force of social evolution in the age of information and artificial intelligence. Technology currently effects society in various aspects. We communicate with others through internet, follow the news from online mediums, create our social identities on virtual environments such as Facebook and Instagram. Our ideas, thoughts and decision-making processes are heavily built upon what we observe and communicate online. 

In other words, our social identities are densely associated with technological advances. Even though this means that the world is connected more than ever, there are critical societal complications which remain unsolved and are needed to be revealed. This essay will attempt to pinpoint three upcoming challenges that society is facing in this regard: the relationship between power and technology, reliability and accountability of information, and maintaining justice and fairness in times of artificial intelligence. Each of these issues are needed to be discussed and solved through negotiations between law makers and the technology makers. 

### Technology and Power
Technology can change the power levels of both citizens and governments, though currently it seems that balance is distorted towards governments. Through technology, citizens can become more educated, prosperous and free, whereas governments can become more effective and responsive to their citizens’ demands and concerns. The conflict, however, between the drive for more freedom and country’s security impairs the power balance. One of the best examples for that can be China’s internal mass surveillance policy towards their citizens. The government monitors them in all possible ways through technology:  follows their social media accounts, traces their internet footprints, uses facial recognition systems on public and private surveillance cameras. They claim to execute this policy for the security of the state, but is it really conducted in a humanitarian and transparent way? We hear how citizens’ freedom of speech is suppressed on social media every day. People who freely state their thoughts against government get punished by losing their “Social Credits” (Zhong 2019). Moreover, the facial recognition technology is used to profile and categorize the minority groups such as Uighurs and Tibetans, who are then thrown into “re-education camps” and behaved in unethical ways (Mozur 2019). 

Government’s power over their citizens by means of technology and AI is not only observed in China, on the contrary, it is becoming very common on many countries. For example, recently, an innocent black United States citizen named George Floyd was killed by a police officer on the street and this incident triggered the global-level protests of “Black Lives Matter”.  Countless amounts of protests took place on the streets of many countries, such as US, Canada, Belgium, UK, Germany… These protests are targeting the structural racism within the society and government and are not really welcomed by the government nor the police. Hence, some parties raised awareness to the risk of governments using facial recognition to identify the protestors. Several tech companies already announced that they ceased selling that technology, yet there are already many startups that work with governments closely (Reichert 2020). 

Another interesting example of government’s power over citizens by means of technology was the Cambridge Analytica Data Scandal in 2018. The social media company Facebook illegally gave access of its users data to a company named “Cambridge Analytica”, which then harvested users identities for targeted political advertisement, in collaboration with political campaigns of Donald Trump and Ted Cruz (Chan 2020). After Donald Trump was elected as a president, as the government held the control of law, there were no serious investigations nor sanctions against Facebook and the case was closed. However, it shall be clear now with this scandal and the previous examples of government’s power with technology, serious problems arise within the society with the speed of technological advancements. These problems should be pointed out and clarified through policy makers and governments. Governments should use technology in a transparent way, establish a trustworthy ground, and inform citizens about used technologies. Policy makers should be aware of all the potential threats and complications of such technologies, consider all ethical facets to balance the power between citizens and governments.

### Fake News and Filter Bubbles
The second challenge that humanity faces, within the increasing speed of internet and the developments of natural language understanding by artificial intelligence, is the question of accountability and reliability of the information. Most of the people follow the news and gather the information from the internet through online newsletters, Twitter, social media and other circles that they follow. What people read effect what they think, how they vote, how their perspectives towards domestic and international politics are. This creates two potential threats: vulnerability to malicious parties by fake news, and the filter bubbles. 

Fake news is the fraudulent information and stories disguised as legitimate news stories which can be generated either manually by people, or by artificial intelligence. AI systems that focus on language understanding and generating are improving fast. There are systems that can identify a person’s political view by looking at their online identity, shared posts and pictures. Fake news is then generated by specifically targeting that person, so that it is an attractive read and it matches with her/his political bias. It is usually convincing and creates a “sensible” ground for that person. In this way, perception management becomes easy for any malicious party which aim to attack and harass others by creating a public opinion through fake news against them. One of the most dramatic usage of fake news was during the Cambridge Analytica Scandal, where some political campaigners generated fake news and disinformation about the other candidates, such as Hilary Clinton, to create public hate and bias against them (Chan 2020). Currently, as fake news can be used easily for personal or financial reasons, several news companies created portals that show the circulated fake news to inform their readers (BBC 2020). 

The distribution of fake news and the selection of target readers are commonly based on “filter bubbles” or in other words, online social bubbles. “Filter bubbles” is a term proposed by Eli Parsier, to describe the paradox between personalized user experience and isolated user islands (Parsier 2011). Such social bubbles occur due to recommendation algorithms that aim to generate personalized advertisements for users, either brand advertisements or the news that our “Twitter feed” shows to us. These algorithms might be useful for getting relevant book or movie recommendations; however, they also heavily restrict the stories that we read from different communities. It can be considered as one of the biggest threats to democracy because they limit citizens’ understanding of different perspectives and judgement mechanism, divide people into sets and cliques, and thicken the invisible boundaries between social groups. Afterwards, fake news can be generated by targeting specific filter bubbles, learning their social identities and polarizing the political perspectives further. Hence, it is unquestionable that both fake news and the filter bubbles have unprecedented effects on democracy by means of perception management. A healthy democracy relies on reliable and free information, and on learning and understanding others. Law makers and technologists should discuss how to resolve this problem, detect the potential threats of fake news, predict the major incidents and risky situations, and find ways to prevent and lessen the effect of both filter bubbles and fake news. 

### Biased Algorithms
The last but not least important socio-technical challenge is the biased AI-based decision making and automatization systems. Decision making process is started to become automatized through AI algorithms, and being more prevalent in recruitment, local government and financial services. These systems use large datasets to learn and apply human-like decisions. However, the existing datasets are naturally created by people, and they contain all the bias that humans have towards genders, race, certain groups etc…  Hence, the learned algorithms reflect the existing societal bias while taking important decisions. In other words, they amplify the biases. 

Two most recent examples on biased algorithms were in AI based facial recognition and classification systems. In 2018, Amazon’s facial recognition tools incorrectly identified 28 black people as criminals resulting in an unjust arrest by police (Romm 2018). AI system misclassified these citizens as criminals, because they were trained on the existing facial datasets of criminals, which happened to be mostly black people, and the algorithm gained bias against black people. This mistake is actually cannot be counted on behalf of AI, but the people who legalize and support such a “criminal classifier” system. Very recently, AI researchers asserted that scientific publishers help perpetuate racist algorithms, and they called Springer Nature with an open letter, requesting the revocation of such papers (Hao 2020). The second similar case took place in one of the top AI conferences, IEEE CVPR, this year. One of the papers there focused on image resolution recovery. They start with a blurred facial image and recover the details of the face to be able to identify the person. Yet interestingly, when this model was fed with a blurred image of the former US president Obama, it generated a face of a white man. These examples visually demonstrate and clarify the existence of the algorithmic bias, however there are many more incidents that show prejudices against minority communities.

Maintaining justice and fairness in times of AI is in the hands of law makers and technologists, not in datasets and algorithms. Many issues in fairness, accountability, transparency and ethics are rooted in decisions surrounding the data collection, annotation and ownership practices. Mimi Onuoha, a professor and researcher at NYU, told that “Every data set involving people implies subjects and objects, those who collect and those who make up the collected. It is imperative to remember that on both sides we have humans”  (Onuoha 2016). Such societal biases enter when we formulate what problems to work on, collect training and evaluation data, architect AI models and loss functions, and analyze how models are used in the society. In order to prevent biased algorithms, datasets should be collected carefully from public community archives, include the long-tail parts of distributions, prioritize the data collection from minority groups and should be an active process to prevent time-dependent bias as well. There are many aspects to consider in the process of aiming unbiased and fair algorithms, and it is indeed an inherent and difficult problem which has only recently started to be discussed. It now imperative for law makers, sociologists, ethics specialists, and researchers, to stay connected, meet up on a mutual ground, reconcile the policies and guidelines to be followed to maintain justice and fairness.  



To sum up, technology is in the center of the society and communication. Yet, there are crucial issues which remain unsolved and can only solved by an in-depth analysis and consultation between law makers and technologists. First, there is an invisible cord between power and technology: currently, governments use technology as a way to puppeteer the citizens through surveillance and secret settlements with tech companies that violate privacy and ethics. Second, as the information and media are centered on online channels, the threat of fake news generation that target individuals and filter bubbles for perception manipulation is tremendously alarming.  Finally, AI based decision making systems become more prevalent every day, yet these systems are racist and sexist because of the datasets that reflect the structural racism and bias deeply knitted in the society. In order to maintain the peace, justice and freedom in society and protect the human rights in our rapidly changing world, law makers and scientists should put their heads together imminently and seek solutions for these three upcoming issues. Otherwise, human rights will not be able to catch the speed of technology and the problems will grow as an avalanche.


### Bibliography

Zhong, Yuhao. 2019. "Rethinking the Social Credit System: A Long Road to Establishing Trust in Chinese Society." Privaci.info. 28, 29.
Mozur, Paul. 2019. One Month, 500,000 Face Scans: How China Is Using A.I. to Profile a Minority. April 14. https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html.
Reichert, Corinne. 2020. Senator fears Clearview AI facial recognition could be used on protesters. June. https://www.cnet.com/news/senator-concerned-clearview-ai-facial-recognition-is-being-used-by-police-in-black-lives-matter-protests/.
Chan, Rosalie. 2020. The Cambridge Analytica whistleblower explains how the firm used Facebook data to sway elections. May. https://www.businessinsider.com/cambridge-analytica-whistleblower-christopher-wylie-facebook-data-2019-10.
BBC. 2020. Fake News Portal. 2020.
Parsier, Eli. 2011. The Filter Bubble: What the Internet is Hiding from You. The Penguin Group.
Romm, Tony. 2018. Amazon’s facial-recognition tool misidentified 28 lawmakers as people arrested for a crime, study finds. July 26. https://www.washingtonpost.com/technology/2018/07/26/amazons-facial-recognition-tool-misidentified-lawmakers-people-arrested-crime-study-finds/.
Hao, Karen. 2020. AI researchers say scientific publishers help perpetuate racist algorithms. June. https://www.technologyreview.com/2020/06/23/1004333/ai-science-publishers-perpetuate-racist-face-recognition/.
Onuoha, Mimi. 2016. The Point of Collection. February 10. https://points.datasociety.net/the-point-of-collection-8ee44ad7c2fa.


